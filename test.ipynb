{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb24d8d",
   "metadata": {},
   "source": [
    "### –†–∞–±–æ—Ç–∞ —Å ollama –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28d49d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "ollama_url = '192.168.8.86'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34d2ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k0/w5_bdr7169jfn7q2nltd0bz00000gn/T/ipykernel_78369/1096076164.py:1: ResourceWarning: unclosed <socket.socket fd=74, family=2, type=1, proto=6, laddr=('192.168.8.183', 50855), raddr=('192.168.8.86', 11434)>\n",
      "  llm =  OllamaLLM(model=\"llama3.2:1b\", temperature=1, base_url=f\"{ollama_url}:11434\")\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. I'm just a language model, I don't have personal feelings or emotions, but I can provide information about cats if you're interested! Many people adore cats for their playful personalities, soft fur, and affectionate nature. Do you have a feline friend at home?\n"
     ]
    }
   ],
   "source": [
    "llm =  OllamaLLM(model=\"llama3.2:1b\", temperature=1, base_url=f\"{ollama_url}:11434\")\n",
    "response = llm.invoke(\"hi, do u love cats\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79307bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k0/w5_bdr7169jfn7q2nltd0bz00000gn/T/ipykernel_78369/1192277074.py:1: ResourceWarning: unclosed <socket.socket fd=75, family=2, type=1, proto=6, laddr=('192.168.8.183', 50856), raddr=('192.168.8.86', 11434)>\n",
      "  embeddings = OllamaEmbeddings(model=\"all-minilm:33m\",  base_url=f\"{ollama_url}:11434\")\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.19035119, 0.27979231],\n",
       "       [0.19035119, 1.        , 0.68131581],\n",
       "       [0.27979231, 0.68131581, 1.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"all-minilm:33m\",  base_url=f\"{ollama_url}:11434\")\n",
    "\n",
    "texts = [\n",
    "    \"–ß—Ç–æ —Ç–∞–∫–æ–µ LangChain?\",\n",
    "    \"–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Ollama –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤?\",\n",
    "    \"–ö–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Ollama?\"\n",
    "]\n",
    "\n",
    "vectors = embeddings.embed_documents(texts)\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "new_matr = vectors/np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "new_matr @ new_matr.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005977d9",
   "metadata": {},
   "source": [
    "### –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23da6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.config import Configure\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "import requests, json, os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edd8f9a",
   "metadata": {},
   "source": [
    "#### –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cbf243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.connect_to_local(\n",
    "    host=ollama_url,  # –£–∫–∞–∂–∏—Ç–µ –∞–¥—Ä–µ—Å —Ö–æ—Å—Ç–∞\n",
    "    port=8080,         # –£–∫–∞–∂–∏—Ç–µ –ø–æ—Ä—Ç HTTP\n",
    "    grpc_port=50051    # –£–∫–∞–∂–∏—Ç–µ –ø–æ—Ä—Ç gRPC\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7d90f",
   "metadata": {},
   "source": [
    "#### –ö–æ–ª–ª–µ–∫—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc657bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n",
    "client.collections.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58979678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–º. –î–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤\n",
    "questions = client.collections.create(\n",
    "    name=\"Question\",\n",
    "    #vectorizer_config=Configure.Vectorizer.text2vec_ollama(     # Configure the Ollama embedding integration\n",
    "    #    api_endpoint=\"http://ollama:11434\",       # Allow Weaviate from within a Docker container to contact your Ollama instance\n",
    "    #    model=\"all-minilm:33m\",                              # The model to use\n",
    "    #),\n",
    "    vectorizer_config =[\n",
    "        Configure.NamedVectors.text2vec_ollama(\n",
    "            name=\"title_vector\",\n",
    "            source_properties=[\"answer\"],\n",
    "            api_endpoint=\"http://ollama:11434\",  # If using Docker, use this to contact your local Ollama instance\n",
    "            model=\"all-minilm:33m\",  # The model to use, e.g. \"nomic-embed-text\"\n",
    "    )\n",
    "    ],\n",
    "    generative_config=Configure.Generative.ollama(              # Configure the Ollama generative integration\n",
    "        api_endpoint=\"http://ollama:11434\",       # Allow Weaviate from within a Docker container to contact your Ollama instance\n",
    "        model=\"llama3.2:1b\",                                       # The model to use\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8676013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞–ø–æ–ª–Ω–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é\n",
    "resp = requests.get(\n",
    "    \"https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json\"\n",
    ")\n",
    "data = json.loads(resp.text)\n",
    "\n",
    "questions = client.collections.get(\"Question\")\n",
    "\n",
    "with questions.batch.dynamic() as batch:\n",
    "    for d in data:\n",
    "        batch.add_object({\n",
    "            \"answer\": d[\"Answer\"],\n",
    "            \"question\": d[\"Question\"],\n",
    "            \"category\": d[\"Category\"],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fad657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"Elephant\",\n",
      "  \"question\": \"It's the only living mammal in the order Proboseidea\",\n",
      "  \"category\": \"ANIMALS\"\n",
      "}\n",
      "{\n",
      "  \"answer\": \"the nose or snout\",\n",
      "  \"question\": \"The gavial looks very much like a crocodile except for this bodily feature\",\n",
      "  \"category\": \"ANIMALS\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–æ–≤–µ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –ø–æ–ª—é answer\n",
    "questions = client.collections.get(\"Question\")\n",
    "\n",
    "response = questions.query.near_text(\n",
    "    query=\"Elephant\",\n",
    "    limit=2\n",
    ")\n",
    "\n",
    "for obj in response.objects:\n",
    "    print(json.dumps(obj.properties, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91cb58e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"Elephant\",\n",
      "  \"question\": \"It's the only living mammal in the order Proboseidea\",\n",
      "  \"category\": \"ANIMALS\"\n",
      "}\n",
      "{\n",
      "  \"answer\": \"the nose or snout\",\n",
      "  \"question\": \"The gavial looks very much like a crocodile except for this bodily feature\",\n",
      "  \"category\": \"ANIMALS\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–æ–≤–µ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –ø–æ –ø–æ–ª—é answer\n",
    "questions = client.collections.get(\"Question\")\n",
    "\n",
    "response = questions.query.hybrid(\n",
    "    query=\"Elephant\",  # The model provider integration will automatically vectorize the query\n",
    "    limit=2\n",
    ")\n",
    "\n",
    "for obj in response.objects:\n",
    "    print(json.dumps(obj.properties, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7904917c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"üêò Did you know elephants are the only living mammals in the order Proboseidea? üåü And, their iconic noses are actually fused to their face! üëÉ What else do you wonder about these incredible creatures? #elephantfacts #wildlife #animals\"\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞–π—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç, –Ω–∞–∏–±–æ–ª–µ–µ –±–ª–∏–∑–∫–∏–π –ø–æ –ø–æ–ª—é \"answer\" –∫ query, –∏ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ —Ç–≤–∏—Ç —Å –ø–æ–º–æ—â—å—é llama3.2\n",
    "questions = client.collections.get(\"Question\")\n",
    "\n",
    "response = questions.generate.near_text(\n",
    "    query=\"Elephant\",\n",
    "    limit=2,\n",
    "    grouped_task=\"Write a tweet with emojis about these facts.\"\n",
    ")\n",
    "\n",
    "print(response.generated)  # Inspect the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a8573e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
