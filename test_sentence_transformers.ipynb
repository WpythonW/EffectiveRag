{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d84e8b-661e-4997-91f9-918a6728474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # type: ignore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "\n",
    "import weaviate\n",
    "from weaviate.classes.data import DataObject # type: ignore\n",
    "\n",
    "import weaviate.classes.query as wq\n",
    "from weaviate.classes.query import Filter\n",
    "from weaviate.classes.query import Rerank, MetadataQuery\n",
    "from weaviate.classes.config import Property, DataType\n",
    "\n",
    "from weaviate.util import generate_uuid5\n",
    "from weaviate.collections import Collection\n",
    "from weaviate.collections.classes.config import (\n",
    "    Property, Configure, DataType, VectorDistances\n",
    ")\n",
    "\n",
    "from enum import Enum\n",
    "from typing import Dict, Tuple\n",
    "import numpy as np\n",
    "import json\n",
    "from math import floor\n",
    "import gc\n",
    "import weakref\n",
    "import asyncio\n",
    "import time\n",
    "from functools import lru_cache\n",
    "from contextlib import suppress\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Set\n",
    "from dataclasses import dataclass\n",
    "import socket\n",
    "\n",
    "from llmlingua import PromptCompressor\n",
    "\n",
    "ollama_url = 'localhost'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b014f4-5de0-42ce-a96d-4ab8c06ea5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BooksProcessor:\n",
    "    \"\"\"\n",
    "    Processor for managing books in a vector database with different chunk sizes.\n",
    "    Uses singleton pattern to prevent multiple instances and manage resources properly.\n",
    "    \"\"\"\n",
    "    _instance = None\n",
    "    _template = None\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self, ollama_url: str = 'localhost', \n",
    "                 embedding_model_name: str = 'nomic-embed-text',\n",
    "                 wv_port_rest: int = 8080, \n",
    "                 wv_port_grpc: int = 50051):\n",
    "        \"\"\"\n",
    "        Initialize the BooksProcessor.\n",
    "        \n",
    "        Args:\n",
    "            ollama_url: URL for Ollama service\n",
    "            embedding_model_name: Name of the embedding model to use\n",
    "            wv_port_rest: Weaviate REST API port\n",
    "            wv_port_grpc: Weaviate gRPC port\n",
    "        \"\"\"\n",
    "        # Initialize only once due to singleton pattern\n",
    "        if not hasattr(self, '_initialized'):\n",
    "            self.embedding_model_name = embedding_model_name\n",
    "            self.ollama_url = ollama_url\n",
    "            self.wv_port_rest = wv_port_rest\n",
    "            self.wv_port_grpc = wv_port_grpc\n",
    "            self.wv_client = None\n",
    "            self._transports = weakref.WeakSet()\n",
    "            self._initialized = True\n",
    "\n",
    "    def _track_transport(self, transport) -> None:\n",
    "        \"\"\"Add transport to tracking set for cleanup\"\"\"\n",
    "        if transport and hasattr(transport, 'close'):\n",
    "            self._transports.add(transport)\n",
    "\n",
    "    def _cleanup_transports(self) -> None:\n",
    "        \"\"\"Clean up all tracked transports\"\"\"\n",
    "        for transport in list(self._transports):\n",
    "            with suppress(Exception):\n",
    "                if not transport.is_closing():\n",
    "                    transport.close()\n",
    "        self._transports.clear()\n",
    "        gc.collect()\n",
    "\n",
    "    def _ensure_weaviate(self) -> None:\n",
    "        \"\"\"Ensure Weaviate connection exists and is properly tracked\"\"\"\n",
    "        if self.wv_client is None:\n",
    "            self.wv_client = weaviate.connect_to_local(\n",
    "                host=self.ollama_url,\n",
    "                port=self.wv_port_rest,\n",
    "                grpc_port=self.wv_port_grpc,\n",
    "            )\n",
    "            \n",
    "            # Track all possible transport variations\n",
    "            if hasattr(self.wv_client, '_connection'):\n",
    "                if hasattr(self.wv_client._connection, 'transport'):\n",
    "                    self._track_transport(self.wv_client._connection.transport)\n",
    "                if hasattr(self.wv_client._connection, '_transport'):\n",
    "                    self._track_transport(self.wv_client._connection._transport)\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close all connections and clean up resources\"\"\"\n",
    "        if self.wv_client is not None:\n",
    "            try:\n",
    "                self.wv_client.close()\n",
    "            finally:\n",
    "                self.wv_client = None\n",
    "        \n",
    "        self._cleanup_transports()\n",
    "        time.sleep(0.2)  # Allow time for connections to close\n",
    "        gc.collect()\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry\"\"\"\n",
    "        self._ensure_weaviate()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit\"\"\"\n",
    "        self.close()\n",
    "\n",
    "    def create_collection_if_not_exists(self, collection_name: str) -> Collection:\n",
    "        \"\"\"\n",
    "        Create a new collection if it doesn't exist, or get existing one.\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name for the collection\n",
    "            \n",
    "        Returns:\n",
    "            Weaviate collection object\n",
    "        \"\"\"\n",
    "        self._ensure_weaviate()\n",
    "        \n",
    "        try:\n",
    "            if self.wv_client.collections.exists(collection_name):\n",
    "                print(f\"Getting '{collection_name}'\")\n",
    "            else:\n",
    "                print(f\"Creating '{collection_name}'\")\n",
    "                self.wv_client.collections.create(\n",
    "                    name=collection_name,\n",
    "                    properties=[\n",
    "                        Property(name=\"chunk\", data_type=DataType.TEXT),\n",
    "                        Property(name=\"book_name\", data_type=DataType.TEXT),\n",
    "                        Property(name=\"chunk_num\", data_type=DataType.INT)\n",
    "                    ],\n",
    "                    vectorizer_config=[\n",
    "                        Configure.NamedVectors.text2vec_ollama(\n",
    "                            name=\"book_vectorizer\",\n",
    "                            source_properties=[\"book_chunks\"],\n",
    "                            api_endpoint=\"http://ollama:11434\",\n",
    "                            model=self.embedding_model_name,\n",
    "                            vector_index_config=Configure.VectorIndex.hnsw(\n",
    "                                distance_metric=VectorDistances.COSINE\n",
    "                            )\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            return self.wv_client.collections.get(collection_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in create_collection_if_not_exists: {e}\")\n",
    "            raise\n",
    "\n",
    "    def split_book(self, book_text: str, chunk_size: int, chunk_overlap: int) -> List:\n",
    "        \"\"\"\n",
    "        Split book text into chunks.\n",
    "        \n",
    "        Args:\n",
    "            book_text: Full text of the book\n",
    "            chunk_size: Size of each chunk\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            \n",
    "        Returns:\n",
    "            List of document chunks\n",
    "        \"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        return splitter.create_documents([book_text])\n",
    "\n",
    "    def send_to_db(self, collection: Collection, chunks: List, book_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Send chunks to the database.\n",
    "        \n",
    "        Args:\n",
    "            collection: Weaviate collection\n",
    "            chunks: List of document chunks\n",
    "            book_name: Name of the book\n",
    "        \"\"\"\n",
    "        with collection.batch.fixed_size(batch_size=10) as batch:\n",
    "            for i, d in enumerate(chunks):\n",
    "                batch.add_object({\n",
    "                    \"chunk\": d.page_content,\n",
    "                    \"book_name\": book_name,\n",
    "                    \"chunk_num\": int(i)\n",
    "                })\n",
    "\n",
    "    def process_book(self, book_name: str, book_txt: str) -> None:\n",
    "        \"\"\"\n",
    "        Process a book by splitting it into chunks and storing in the database.\n",
    "        \n",
    "        Args:\n",
    "            book_name: Name of the book\n",
    "            book_txt: Full text of the book\n",
    "        \"\"\"\n",
    "        self._ensure_weaviate()\n",
    "        \n",
    "        try:\n",
    "            if self.wv_client.collections.exists(book_name + '_big_chunks'):\n",
    "                print(\"Book already exists\")\n",
    "                return\n",
    "\n",
    "            print(\"Processing book\")\n",
    "            \n",
    "            # Process different chunk sizes\n",
    "            chunk_configs = [\n",
    "                ('_big_chunks', 3000, 1000),\n",
    "                ('_medium_chunks', 1500, 500),\n",
    "                ('_small_chunks', 750, 250)\n",
    "            ]\n",
    "            \n",
    "            for suffix, chunk_size, overlap in chunk_configs:\n",
    "                collection = self.create_collection_if_not_exists(book_name + suffix)\n",
    "                chunks = self.split_book(book_txt, chunk_size, overlap)\n",
    "                self.send_to_db(collection, chunks, book_name)\n",
    "                gc.collect()  # Clean up after each major operation\n",
    "\n",
    "            print(\"Book successfully processed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_book: {e}\")\n",
    "            raise\n",
    "\n",
    "    def delete_book(self, book_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Delete all collections associated with a book.\n",
    "        \n",
    "        Args:\n",
    "            book_name: Name of the book to delete\n",
    "        \"\"\"\n",
    "        self._ensure_weaviate()\n",
    "        \n",
    "        for suffix in ['_big_chunks', '_medium_chunks', '_small_chunks']:\n",
    "            try:\n",
    "                self.wv_client.collections.delete(book_name + suffix)\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting collection {book_name}{suffix}: {e}\")\n",
    "        \n",
    "        print(f\"Successfully deleted collections for {book_name}\")\n",
    "\n",
    "\n",
    "class ChunkSize(Enum):\n",
    "    SMALL = '_small_chunks'\n",
    "    MEDIUM = '_medium_chunks'\n",
    "    LARGE = '_big_chunks'\n",
    "\n",
    "class Search:\n",
    "    \"\"\"\n",
    "    Search class for querying book content with proper resource management.\n",
    "    Uses singleton pattern to prevent multiple instances.\n",
    "    \"\"\"\n",
    "    _instance = None\n",
    "    _prompt_template = None\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self, ollama_url: str = 'localhost', llm_name: str = 'Llama3.2',\n",
    "                 wv_port_rest: int = 8080, wv_port_grpc: int = 50051):\n",
    "        \"\"\"\n",
    "        Initialize the Search instance.\n",
    "        \n",
    "        Args:\n",
    "            ollama_url: URL for Ollama service\n",
    "            llm_name: Name of the LLM model to use\n",
    "            wv_port_rest: Weaviate REST API port\n",
    "            wv_port_grpc: Weaviate gRPC port\n",
    "        \"\"\"\n",
    "        if not hasattr(self, '_initialized'):\n",
    "            self.ollama_url = ollama_url\n",
    "            self.llm_name = llm_name\n",
    "            self.wv_port_rest = wv_port_rest\n",
    "            self.wv_port_grpc = wv_port_grpc\n",
    "            self.llm = None\n",
    "            self.wv_client = None\n",
    "            self._transports = weakref.WeakSet()\n",
    "            self._initialized = True\n",
    "            self._load_prompt_template()\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=1)\n",
    "    def _load_prompt_template() -> None:\n",
    "        \"\"\"Load and cache the prompt template\"\"\"\n",
    "        if Search._prompt_template is None:\n",
    "            with open('classifier_prompt.j2') as f:\n",
    "                template = f.read()\n",
    "            Search._prompt_template = PromptTemplate(\n",
    "                input_variables=[\"query\"],\n",
    "                template=template,\n",
    "                template_format=\"jinja2\"\n",
    "            )\n",
    "\n",
    "    def _track_transport(self, transport) -> None:\n",
    "        \"\"\"Add transport to tracking set for cleanup\"\"\"\n",
    "        if transport and hasattr(transport, 'close'):\n",
    "            self._transports.add(transport)\n",
    "\n",
    "    def _cleanup_transports(self) -> None:\n",
    "        \"\"\"Clean up all tracked transports\"\"\"\n",
    "        for transport in list(self._transports):\n",
    "            with suppress(Exception):\n",
    "                if not transport.is_closing():\n",
    "                    transport.close()\n",
    "        self._transports.clear()\n",
    "        gc.collect()\n",
    "\n",
    "    def _ensure_llm(self) -> None:\n",
    "        \"\"\"Ensure LLM connection exists\"\"\"\n",
    "        if self.llm is None:\n",
    "            self.llm = OllamaLLM(\n",
    "                model=self.llm_name,\n",
    "                temperature=0,\n",
    "                base_url=f\"http://{self.ollama_url}:11434\"\n",
    "            )\n",
    "            # Track LLM client transport if available\n",
    "            if hasattr(self.llm, 'client') and hasattr(self.llm.client, '_transport'):\n",
    "                self._track_transport(self.llm.client._transport)\n",
    "\n",
    "    def _ensure_weaviate(self) -> None:\n",
    "        \"\"\"Ensure Weaviate connection exists\"\"\"\n",
    "        if self.wv_client is None:\n",
    "            self.wv_client = weaviate.connect_to_local(\n",
    "                host=self.ollama_url,\n",
    "                port=self.wv_port_rest,\n",
    "                grpc_port=self.wv_port_grpc,\n",
    "            )\n",
    "            # Track Weaviate client transports\n",
    "            if hasattr(self.wv_client, '_connection'):\n",
    "                if hasattr(self.wv_client._connection, 'transport'):\n",
    "                    self._track_transport(self.wv_client._connection.transport)\n",
    "                if hasattr(self.wv_client._connection, '_transport'):\n",
    "                    self._track_transport(self.wv_client._connection._transport)\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close all connections and clean up resources\"\"\"\n",
    "        try:\n",
    "            if self.llm is not None:\n",
    "                if hasattr(self.llm, 'client') and hasattr(self.llm.client, 'close'):\n",
    "                    self.llm.client.close()\n",
    "                self.llm = None\n",
    "                \n",
    "            if self.wv_client is not None:\n",
    "                self.wv_client.close()\n",
    "                self.wv_client = None\n",
    "                \n",
    "            self._cleanup_transports()\n",
    "            time.sleep(0.2)  # Allow time for connections to close\n",
    "        finally:\n",
    "            gc.collect()\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry\"\"\"\n",
    "        self._ensure_llm()\n",
    "        self._ensure_weaviate()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit\"\"\"\n",
    "        self.close()\n",
    "\n",
    "    def classify_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Classify the query to determine appropriate chunk size.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            \n",
    "        Returns:\n",
    "            The classified chunk size suffix\n",
    "        \"\"\"\n",
    "        self._ensure_llm()\n",
    "        prompt = self._prompt_template.format(query=query)\n",
    "        response = self.llm.invoke(prompt).strip().upper()\n",
    "        print(f'Chunk size chosen: {response}')\n",
    "        try:\n",
    "            return ChunkSize[response].value\n",
    "        except KeyError:\n",
    "            return ChunkSize.MEDIUM.value\n",
    "\n",
    "    def search(self, query: str, book_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Search for relevant content in the book.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            book_name: Name of the book to search in\n",
    "            \n",
    "        Returns:\n",
    "            Retrieved content from the book\n",
    "        \"\"\"\n",
    "        self._ensure_llm()\n",
    "        self._ensure_weaviate()\n",
    "        \n",
    "        try:\n",
    "            collection_type = self.classify_query(query)\n",
    "            book = self.wv_client.collections.get(book_name + collection_type)\n",
    "\n",
    "            result = book.aggregate.over_all(total_count=True)\n",
    "            total_count = result.total_count\n",
    "            chunks_to_retrieve = floor(np.maximum(1.5 * np.log(total_count), 1))\n",
    "            \n",
    "            response = book.query.hybrid(\n",
    "                query=query,\n",
    "                limit=chunks_to_retrieve,\n",
    "            )\n",
    "            \n",
    "            print(f'Chunks retrieved: {chunks_to_retrieve}')\n",
    "            relevant_chunks = sorted(response.objects, key=lambda x: x.properties['chunk_num'])\n",
    "            return '\\n\\n'.join([i.properties['chunk'].strip() for i in relevant_chunks])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in search: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for RAG system\"\"\"\n",
    "    compression_rate: float = 0.75\n",
    "    force_tokens: List[str] = None\n",
    "    llmlingua_model: str = \"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\"\n",
    "    ollama_model: str = \"llama3.2\"\n",
    "    ollama_url: str = \"localhost\"\n",
    "    temperature: float = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.force_tokens is None:\n",
    "            self.force_tokens = ['\\n', '?', '.', '!']\n",
    "\n",
    "class SocketManager:\n",
    "    \"\"\"Manager for tracking and cleaning up sockets\"\"\"\n",
    "    def __init__(self):\n",
    "        self.sockets: Set[socket.socket] = set()\n",
    "        self._original_socket = socket.socket\n",
    "        self._patch_socket()\n",
    "        \n",
    "    def _patch_socket(self):\n",
    "        \"\"\"Patch socket creation to track all sockets\"\"\"\n",
    "        def _tracked_socket(*args, **kwargs):\n",
    "            sock = self._original_socket(*args, **kwargs)\n",
    "            self.sockets.add(sock)\n",
    "            return sock\n",
    "            \n",
    "        socket.socket = _tracked_socket\n",
    "        \n",
    "    def _unpatch_socket(self):\n",
    "        \"\"\"Restore original socket\"\"\"\n",
    "        socket.socket = self._original_socket\n",
    "        \n",
    "    def cleanup(self):\n",
    "        \"\"\"Close all tracked sockets\"\"\"\n",
    "        for sock in list(self.sockets):\n",
    "            try:\n",
    "                if not sock._closed:\n",
    "                    sock.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.sockets.clear()\n",
    "        self._unpatch_socket()\n",
    "\n",
    "class ResourceManager:\n",
    "    \"\"\"Manager for all system resources\"\"\"\n",
    "    def __init__(self):\n",
    "        self.socket_manager = SocketManager()\n",
    "        self._transports = weakref.WeakSet()\n",
    "        \n",
    "    def track_transport(self, obj):\n",
    "        \"\"\"Track any transports associated with an object\"\"\"\n",
    "        if hasattr(obj, '_transport'):\n",
    "            self._transports.add(obj._transport)\n",
    "        if hasattr(obj, 'transport'):\n",
    "            self._transports.add(obj.transport)\n",
    "        if hasattr(obj, 'client'):\n",
    "            if hasattr(obj.client, '_transport'):\n",
    "                self._transports.add(obj.client._transport)\n",
    "            if hasattr(obj.client, 'transport'):\n",
    "                self._transports.add(obj.client.transport)\n",
    "                \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up all resources\"\"\"\n",
    "        # Clean transports\n",
    "        for transport in list(self._transports):\n",
    "            try:\n",
    "                if hasattr(transport, 'close') and not transport.is_closing():\n",
    "                    transport.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "        self._transports.clear()\n",
    "        \n",
    "        # Clean sockets\n",
    "        self.socket_manager.cleanup()\n",
    "        \n",
    "        # Clean event loop\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            if not loop.is_closed():\n",
    "                for task in asyncio.all_tasks(loop):\n",
    "                    task.cancel()\n",
    "                loop.run_until_complete(asyncio.sleep(0.1))\n",
    "                loop.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "        # Force cleanup\n",
    "        time.sleep(0.2)\n",
    "        gc.collect()\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"RAG system with proper resource management\"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self, config: Optional[RAGConfig] = None):\n",
    "        if not hasattr(self, '_initialized'):\n",
    "            self.config = config or RAGConfig()\n",
    "            self._initialized = True\n",
    "            self.resource_manager = ResourceManager()\n",
    "            self.compressor = None\n",
    "            self.llm = None\n",
    "            self._template = None\n",
    "            self._initialize_resources()\n",
    "\n",
    "    def _initialize_resources(self):\n",
    "        \"\"\"Initialize all required resources\"\"\"\n",
    "        if self.compressor is None:\n",
    "            self.compressor = PromptCompressor(\n",
    "                model_name=self.config.llmlingua_model,\n",
    "                use_llmlingua2=True,\n",
    "                device_map=\"cpu\"\n",
    "            )\n",
    "            self.resource_manager.track_transport(self.compressor)\n",
    "        \n",
    "        if self.llm is None:\n",
    "            self.llm = OllamaLLM(\n",
    "                model=self.config.ollama_model,  # было llama_model, исправлено на ollama_model\n",
    "                temperature=self.config.temperature,\n",
    "                base_url=f\"http://{self.config.ollama_url}:11434\"\n",
    "            )\n",
    "            self.resource_manager.track_transport(self.llm)\n",
    "        \n",
    "        if self._template is None:\n",
    "            with open('final_prompt.j2') as f:\n",
    "                self._template = f.read()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Cleanup all resources\"\"\"\n",
    "        try:\n",
    "            if self.compressor is not None:\n",
    "                if hasattr(self.compressor, 'close'):\n",
    "                    self.compressor.close()\n",
    "                self.compressor = None\n",
    "            \n",
    "            if self.llm is not None:\n",
    "                if hasattr(self.llm, 'client') and hasattr(self.llm.client, 'close'):\n",
    "                    self.llm.client.close()\n",
    "                self.llm = None\n",
    "            \n",
    "            self.resource_manager.cleanup()\n",
    "            self._initialized = False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()\n",
    "\n",
    "    def _get_context(self, query: str, book_name: str) -> str:\n",
    "        \"\"\"Get RAG context for a single book\"\"\"\n",
    "        with Search() as searcher:\n",
    "            return searcher.search(query, book_name)\n",
    "            \n",
    "    def _compress_context(self, context: str) -> str:\n",
    "        \"\"\"Compress context using LLMLingua\"\"\"\n",
    "        if not context:\n",
    "            return \"\"\n",
    "        result = self.compressor.compress_prompt(\n",
    "            context,\n",
    "            rate=self.config.compression_rate,\n",
    "            force_tokens=self.config.force_tokens\n",
    "        )\n",
    "        return result['compressed_prompt']\n",
    "        \n",
    "    def _format_final_prompt(self, \n",
    "                           compressed_contexts: List[str],\n",
    "                           dialogue_history: List[Dict[str, str]],\n",
    "                           query: str) -> str:\n",
    "        \"\"\"Format final prompt using Jinja2 template\"\"\"\n",
    "        from jinja2 import Template\n",
    "        template = Template(self._template)\n",
    "        return template.render(\n",
    "            contexts=compressed_contexts,\n",
    "            dialogue_history=dialogue_history,\n",
    "            query=query\n",
    "        )\n",
    "        \n",
    "    def query(self, \n",
    "             query: str,\n",
    "             book_names: List[str],\n",
    "             dialogue_history: Optional[List[Dict[str, str]]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Execute RAG query across multiple books with dialogue history.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._initialize_resources()  # Ensure resources are initialized\n",
    "            dialogue_history = dialogue_history or []\n",
    "            \n",
    "            # Get and compress context from each book\n",
    "            compressed_contexts = []\n",
    "            for book_name in book_names:\n",
    "                try:\n",
    "                    context = self._get_context(query, book_name)\n",
    "                    if context:\n",
    "                        compressed = self._compress_context(context)\n",
    "                        compressed_contexts.append(f\"From {book_name}:\\n{compressed}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing book {book_name}: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "            if not compressed_contexts:\n",
    "                return \"I couldn't find relevant information in the provided books.\"\n",
    "                \n",
    "            # Format final prompt\n",
    "            final_prompt = self._format_final_prompt(\n",
    "                compressed_contexts=compressed_contexts,\n",
    "                dialogue_history=dialogue_history,\n",
    "                query=query\n",
    "            )\n",
    "            \n",
    "            # Get LLM response\n",
    "            response = self.llm.invoke(final_prompt)\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in query: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2321160a-077e-4472-b7f8-587cc225aeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book already exists\n"
     ]
    }
   ],
   "source": [
    "# Способ 2: напрямую через контекстный менеджер\n",
    "with BooksProcessor() as processor:\n",
    "    with open('Sherlock Study in Scarlet.txt', 'r', encoding='utf8') as file:\n",
    "        text = file.read()\n",
    "    processor.process_book('Sherlock_Study_in_Scarlet', text)\n",
    "    #processor.delete_book('Sherlock_Study_in_Scarlet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca183a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = Search()\n",
    "query = \"Who was the criminal?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1842c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size chosen: SMALL\n",
      "Chunks retrieved: 9\n"
     ]
    }
   ],
   "source": [
    "rag_context = search.search(query=query, book_name='Sherlock_Study_in_Scarlet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c86d91a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size chosen: MEDIUM\n",
      "Chunks retrieved: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ustin\\AppData\\Local\\Temp\\ipykernel_21696\\1805635186.py:284: ResourceWarning: unclosed <socket.socket fd=2168, family=23, type=1, proto=0, laddr=('::1', 60772, 0, 0), raddr=('::1', 11434, 0, 0)>\n",
      "  gc.collect()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2902 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context from \"Sherlock_Study_in_Scarlet\", it appears that Sherlock Holmes and Dr. Watson were discussing various cases they had worked on.\n",
      "\n",
      "One case involved a man named Mr. Drebber, who was found dead under mysterious circumstances. Mrs. Charpentier reported that her son, Lieutenant Charpentier, returned home around 11 pm, but she didn't know what he did during the two hours he was gone. Holmes believed this was the key to solving the case and arrested Lieutenant Charpentier.\n",
      "\n",
      "Additionally, there was an incident involving a group of dirty street Arabs who were sent by Wiggins, a member of the Baker Street division of detective police force, to report on some suspicious activity. However, they didn't find anything significant.\n",
      "\n",
      "It's also mentioned that Holmes had previously dealt with a case involving a man named Mr. Drebber, but the details are not provided in this context.\n",
      "\n",
      "There is no mention of any other significant events or happenings in London beyond these cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ustin\\AppData\\Local\\Temp\\ipykernel_21696\\1805635186.py:481: RuntimeWarning: coroutine 'sleep' was never awaited\n",
      "  pass\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# 1. Сначала у нас есть книги в векторной БД (используем BooksProcessor)\n",
    "#with BooksProcessor() as processor:\n",
    "#    processor.process_book('book1', text1)\n",
    "#    processor.process_book('book2', text2)\n",
    "\n",
    "# 2. Теперь можем использовать RAGSystem для поиска и ответов\n",
    "config = RAGConfig(compression_rate=0.75)\n",
    "rag = RAGSystem(config)\n",
    "\n",
    "try:\n",
    "    response = rag.query(\n",
    "        query=\"What happened in London?\",\n",
    "        book_names=['Sherlock_Study_in_Scarlet'],\n",
    "        dialogue_history=[]\n",
    "    )\n",
    "    print(response)\n",
    "finally:\n",
    "    rag.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37faa111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
