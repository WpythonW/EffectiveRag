services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "${OLLAMA_PORT}:11434"
    environment:
      OLLAMA_MAX_LOADED_MODELS: 2
      OLLAMA_KEEP_ALIVE: -1
    volumes:
      - ./models:/root/.ollama
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - app_network
    entrypoint: |
      bash -c '
      # Start Ollama server
      ollama serve &
      
      # Wait for the server to be ready
      while ! echo > /dev/tcp/localhost/11434 2>/dev/null; do
          echo "Waiting for Ollama server to be ready..."
          sleep 2
      done
      
      echo "Ollama server is ready. Starting to pull models..."
      
      # Pull the models
      if [ ! -z "${LLM}" ]; then
          echo "Pulling ${LLM}..."
          ollama pull ${LLM}
      fi
      
      # Keep the container running
      wait
      '

  weaviate:
    command:
      - --host
      - 0.0.0.0
      - --port
      - '8080'
      - --scheme
      - http
    image: semitechnologies/weaviate
    container_name: weaviate
    ports:
      - ${WEAVIATE_PORT_REST}:8080
      - ${WEAVIATE_PORT_GRPC}:50051
    volumes:
      - ./weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      #DEFAULT_VECTORIZER_MODULE: 'text2vec-ollama'
      #ENABLE_MODULES: 'text2vec-ollama,generative-ollama'
      #OLLAMA_HOST: 'http://ollama:11434'
      #RERANKER_INFERENCE_API: 'http://reranker-transformers:8080'

      CLUSTER_HOSTNAME: 'node1'
    networks:
      - app_network
    depends_on:
      ollama:
        condition: service_healthy

networks:
  app_network:
    driver: bridge
