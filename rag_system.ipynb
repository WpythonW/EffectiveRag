{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d84e8b-661e-4997-91f9-918a6728474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\RAG project\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # type: ignore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "import weaviate\n",
    "\n",
    "import weaviate.classes as wvc\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from weaviate.classes.config import Property, DataType\n",
    "\n",
    "from weaviate.collections import Collection\n",
    "from weaviate.collections.classes.config import (\n",
    "    Property, DataType\n",
    ")\n",
    "\n",
    "from enum import Enum\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from llmlingua import PromptCompressor\n",
    "\n",
    "from jinja2 import Template\n",
    "\n",
    "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True, device='cuda')\n",
    "compressor = PromptCompressor(model_name='microsoft/llmlingua-2-xlm-roberta-large-meetingbank', use_llmlingua2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b014f4-5de0-42ce-a96d-4ab8c06ea5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BooksProcessor:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.wv_client = None\n",
    "\n",
    "    def connect(self):\n",
    "        if not self.wv_client:\n",
    "            self.wv_client = weaviate.connect_to_local()\n",
    "    def close(self):\n",
    "        if self.wv_client:\n",
    "            self.wv_client.close()\n",
    "\n",
    "    def create_collection_if_not_exists(self, collection_name):\n",
    "        self.connect()\n",
    "        if not self.wv_client.collections.exists(collection_name):\n",
    "            self.wv_client.collections.create(\n",
    "                name=collection_name,\n",
    "                properties=[\n",
    "                    Property(name=\"chunk\", data_type=DataType.TEXT),\n",
    "                    Property(name=\"book_name\", data_type=DataType.TEXT),\n",
    "                    Property(name=\"chunk_num\", data_type=DataType.INT)\n",
    "                ],\n",
    "                #vectorizer_config=wvc.config.Configure.Vectorizer.none()\n",
    "                #vectorizer_config=[\n",
    "                    #Configure.NamedVectors.text2vec_ollama(\n",
    "                    #    name=\"book_vectorizer\",\n",
    "                    #    source_properties=[\"book_chunks\"],\n",
    "                    #    api_endpoint=\"http://ollama:11434\",\n",
    "                    #    model=self.embedding_model_name,\n",
    "                    #    vector_index_config=Configure.VectorIndex.hnsw(\n",
    "                    #        distance_metric=VectorDistances.COSINE\n",
    "                    #    )\n",
    "                    #)\n",
    "                #]\n",
    "            )\n",
    "        return self.wv_client.collections.get(collection_name)\n",
    "\n",
    "    def split_book(self, book_text, chunk_size, chunk_overlap):\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        return [i.page_content for i in splitter.create_documents([book_text])]\n",
    "\n",
    "    def process_book(self, book_name, book_txt):\n",
    "        self.connect()\n",
    "        if self.wv_client.collections.exists(book_name + '_big_chunks'):\n",
    "            print(\"Book already exists\")\n",
    "            return\n",
    "        chunk_configs = [\n",
    "            ('_big_chunks', 3000, 1000),\n",
    "            ('_medium_chunks', 1500, 500),\n",
    "            ('_small_chunks', 400, 50)\n",
    "        ]\n",
    "        \n",
    "        for suffix, chunk_size, overlap in chunk_configs:\n",
    "            collection = self.create_collection_if_not_exists(book_name + suffix)\n",
    "            chunks = self.split_book(book_txt, chunk_size, overlap)\n",
    "            embeddings = self.embedding_model.encode(['search_document: ' + i for i in chunks], batch_size=15).tolist()\n",
    "            question_objs = []\n",
    "\n",
    "            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "                question_objs.append(wvc.data.DataObject(\n",
    "                    properties= {\n",
    "                        \"chunk\": chunk,\n",
    "                        \"book_name\": book_name,\n",
    "                        \"chunk_num\": i\n",
    "                    },\n",
    "                    vector=embedding\n",
    "                ))\n",
    "            collection.data.insert_many(question_objs)\n",
    "\n",
    "    def delete_book(self, book_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Delete all collections associated with a book.\n",
    "        \"\"\"\n",
    "        self.connect()\n",
    "        for suffix in ['_big_chunks', '_medium_chunks', '_small_chunks']:\n",
    "            collection_name = book_name + suffix\n",
    "            if self.wv_client.collections.exists(collection_name):\n",
    "                try:\n",
    "                    self.wv_client.collections.delete(collection_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting collection {collection_name}: {e}\")\n",
    "        print(f\"Successfully deleted collections for {book_name}\")\n",
    "\n",
    "class ChunkSize(Enum):\n",
    "    SMALL = '_small_chunks'\n",
    "    MEDIUM = '_medium_chunks'\n",
    "    LARGE = '_big_chunks'\n",
    "\n",
    "class Search:\n",
    "    def __init__(self, embedding_model, llm_name='llama3.2'):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.llm_name = llm_name\n",
    "        self.llm = None\n",
    "        self.wv_client = None\n",
    "        self.multiplier_mapping = {'_big_chunks': 0.7, '_medium_chunks': 1, '_small_chunks': 1.9}\n",
    "        self._load_prompt_template()\n",
    "\n",
    "    def _load_prompt_template(self):\n",
    "        with open('classifier_prompt.j2') as f:\n",
    "            template = f.read()\n",
    "            self._prompt_template = PromptTemplate(\n",
    "                input_variables=[\"query\"],\n",
    "                template=template,\n",
    "                template_format=\"jinja2\"\n",
    "            )\n",
    "\n",
    "    def connect(self):\n",
    "        if not self.wv_client:\n",
    "            self.wv_client = weaviate.connect_to_local()\n",
    "        if not self.llm:\n",
    "            self.llm = OllamaLLM(\n",
    "                model=self.llm_name,\n",
    "                temperature=0,\n",
    "                base_url=f\"http://localhost:11434\"\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.wv_client:\n",
    "            self.wv_client.close()\n",
    "\n",
    "    def classify_query(self, query):\n",
    "        self.connect()\n",
    "        response = self.llm.invoke(self._prompt_template.format(query=query)).strip().upper()\n",
    "        return getattr(ChunkSize, response, ChunkSize.MEDIUM).value\n",
    "\n",
    "    def search(self, query, book_name):\n",
    "        self.connect()\n",
    "        #collection_type = self.classify_query(query)\n",
    "        collection_type = '_medium_chunks'\n",
    "        print(f'Collection type: {collection_type}')\n",
    "        book = self.wv_client.collections.get(book_name + collection_type)#collection_type)\n",
    "        \n",
    "        total_count = book.aggregate.over_all(total_count=True).total_count\n",
    "        chunks_to_retrieve = floor(np.maximum(self.multiplier_mapping[collection_type] * np.log(total_count), 1))\n",
    "        print(f\"Retrieving {chunks_to_retrieve} chunks\")\n",
    "        \n",
    "        embedding = self.embedding_model.encode('search_query: ' + query, batch_size=1)\n",
    "        response = book.query.near_vector(near_vector=list(embedding), limit=chunks_to_retrieve, return_metadata=wvc.query.MetadataQuery(certainty=True))\n",
    "        relevant_chunks = response.objects#sorted(response.objects, key=lambda x: x.properties['chunk_num'])\n",
    "        relevant_text = '\\nCHUNK\\n'.join([i.properties['chunk'].strip() for i in relevant_chunks])\n",
    "        print(f'Len of relevant text: {len(relevant_text)}')\n",
    "        return relevant_text\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, embedding_model, compressor, llm_name='llama3.2', compression_rate=0.75):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.compression_rate = compression_rate\n",
    "        self.compressor = compressor\n",
    "        self.llm = OllamaLLM(\n",
    "            model=llm_name,\n",
    "            temperature=0,\n",
    "            base_url=f\"http://localhost:11434\"\n",
    "        )\n",
    "        with open('final_prompt.j2') as f:\n",
    "            self._template = f.read()\n",
    "\n",
    "    def query(self, query: str, book_names: List[str], \n",
    "             dialogue_history: Optional[List[Dict[str, str]]] = None) -> str:\n",
    "        dialogue_history = dialogue_history or []\n",
    "        compressed_contexts = []\n",
    "        searcher = Search(self.embedding_model)\n",
    "        for book_name in book_names:\n",
    "            context = searcher.search(query, book_name)\n",
    "            if context:\n",
    "                compressed = self.compressor.compress_prompt(\n",
    "                    context,\n",
    "                    rate=self.compression_rate,\n",
    "                    force_tokens=['\\n', '?', '.', '!', '\\nCHUNK\\n']\n",
    "                )['compressed_prompt']\n",
    "                compressed_contexts.append(f\"From {book_name}:\\n{compressed}\")\n",
    "        searcher.close()\n",
    "        \n",
    "        if not compressed_contexts:\n",
    "            return \"No relevant information found.\"\n",
    "\n",
    "        print(f'Len of compressed context: {sum([len(i) for i in compressed_contexts])}')\n",
    "        final_prompt = Template(self._template).render(\n",
    "            contexts=compressed_contexts,\n",
    "            dialogue_history=dialogue_history,\n",
    "            query=query\n",
    "        )\n",
    "        \n",
    "        return self.llm.invoke(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2321160a-077e-4472-b7f8-587cc225aeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book already exists\n"
     ]
    }
   ],
   "source": [
    "# Способ 2: напрямую через контекстный менеджер\n",
    "processor = BooksProcessor(embedding_model)\n",
    "with open('Sherlock Study in Scarlet.txt', 'r', encoding='utf8') as file:\n",
    "    text = file.read()\n",
    "processor.process_book('Sherlock_Study_in_Scarlet', text)\n",
    "#processor.delete_book('Sherlock_Study_in_Scarlet')\n",
    "processor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1842c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection type: _medium_chunks\n",
      "Retrieving 10 chunks\n",
      "Len of relevant text: 3507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'“There has been a bad business during the night at 3, Lauriston\\nGardens, off the Brixton Road. Our man on the beat saw a light there\\nabout two in the morning, and as the house was an empty one, suspected\\nthat something was amiss. He found the door open, and in the front\\nroom, which is bare of furniture, discovered the body of a gentleman,\\nCHUNK\\nhim, and it was argued at the time in the newspapers that the secret\\nsocieties must have done it. I guessed that what puzzled the New\\nYorkers would puzzle the Londoners, so I dipped my finger in my own\\nblood and printed it on a convenient place on the wall. Then I walked\\ndown to my cab and found that there was nobody about, and that the\\nCHUNK\\nThe _Standard_ commented upon the fact that lawless outrages of the\\nsort usually occurred under a Liberal Administration. They arose from\\nthe unsettling of the minds of the masses, and the consequent weakening\\nof all authority. The deceased was an American gentleman who had been\\nresiding for some weeks in the Metropolis. He had stayed at the\\nCHUNK\\nThe papers next day were full of the “Brixton Mystery,” as they termed\\nit. Each had a long account of the affair, and some had leaders upon it\\nin addition. There was some information in them which was new to me. I\\nstill retain in my scrap-book numerous clippings and extracts bearing\\nupon the case. Here is a condensation of a few of them:—\\nCHUNK\\nStation about half-past eight on the evening of the third. At two in\\nthe morning Drebber had been found in the Brixton Road. The question\\nwhich confronted me was to find out how Stangerson had been employed\\nbetween 8.30 and the time of the crime, and what had become of him\\nafterwards. I telegraphed to Liverpool, giving a description of the\\nCHUNK\\nthe platform. Nothing more is known of them until Mr. Drebber’s body\\nwas, as recorded, discovered in an empty house in the Brixton Road,\\nmany miles from Euston. How he came there, or how he met his fate, are\\nquestions which are still involved in mystery. Nothing is known of the\\nwhereabouts of Stangerson. We are glad to learn that Mr. Lestrade and\\nCHUNK\\nfeared that they were going to shift their quarters. At Euston Station\\nthey got out, and I left a boy to hold my horse, and followed them on\\nto the platform. I heard them ask for the Liverpool train, and the\\nguard answer that one had just gone and there would not be another for\\nsome hours. Stangerson seemed to be put out at that, but Drebber was\\nCHUNK\\n“Well, my theory is that he followed Drebber as far as the Brixton\\nRoad. When there, a fresh altercation arose between them, in the course\\nof which Drebber received a blow from the stick, in the pit of the\\nstomach, perhaps, which killed him without leaving any mark. The night\\nwas so wet that no one was about, so Charpentier dragged the body of\\nCHUNK\\n“They were very near doing it for all that. Go where they would about\\nLondon, I was always at their heels. Sometimes I followed them on my\\ncab, and sometimes on foot, but the former was the best, for then they\\ncould not get away from me. It was only early in the morning or late at\\nnight that I could earn anything, so that I began to get behind hand\\nCHUNK\\nI had neither kith nor kin in England, and was therefore as free as\\nair—or as free as an income of eleven shillings and sixpence a day will\\npermit a man to be. Under such circumstances, I naturally gravitated to\\nLondon, that great cesspool into which all the loungers and idlers of\\nthe Empire are irresistibly drained. There I stayed for some time at a'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = Search(embedding_model)\n",
    "rag_context = search.search(query='search_query: ' + 'What happened in London?', book_name='Sherlock_Study_in_Scarlet')\n",
    "search.close()\n",
    "rag_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c86d91a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ustin\\AppData\\Local\\Temp\\ipykernel_21112\\952793858.py:1: ResourceWarning: unclosed <socket.socket fd=4976, family=23, type=1, proto=0, laddr=('::1', 56541, 0, 0), raddr=('::1', 11434, 0, 0)>\n",
      "  rag = RAGSystem(embedding_model, compressor)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection type: _medium_chunks\n",
      "Retrieving 8 chunks\n",
      "Len of relevant text: 9069\n",
      "Len of compressed context: 7217\n",
      "According to the context, it is a \"private wrong, not political one, called for methodical revenge\" and possibly driven by hatred and fear, as indicated by the expression on the face of the murdered man.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ustin\\AppData\\Local\\Temp\\ipykernel_21112\\952793858.py:3: ResourceWarning: unclosed <socket.socket fd=4644, family=23, type=1, proto=0, laddr=('::1', 57121, 0, 0), raddr=('::1', 11434, 0, 0)>\n",
      "  response = rag.query(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "rag = RAGSystem(embedding_model, compressor)\n",
    "\n",
    "response = rag.query(\n",
    "    query=\"What motivates murder\",\n",
    "    book_names=['Sherlock_Study_in_Scarlet'],\n",
    "    dialogue_history=[]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37faa111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
