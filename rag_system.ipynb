{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d84e8b-661e-4997-91f9-918a6728474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\RAG project\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # type: ignore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "import weaviate\n",
    "\n",
    "import weaviate.classes as wvc\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from weaviate.classes.config import Property, DataType\n",
    "\n",
    "from weaviate.collections import Collection\n",
    "from weaviate.collections.classes.config import (\n",
    "    Property, DataType\n",
    ")\n",
    "\n",
    "from enum import Enum\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from typing import List, Dict, Optional\n",
    "from llmlingua import PromptCompressor\n",
    "from jinja2 import Template\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "llm_name = os.getenv(\"LLM\")\n",
    "prompts_folder = os.getenv(\"PROMPTS_FOLDER\")\n",
    "embedding_model_path = os.getenv(\"ENCODER_MODEL\")\n",
    "\n",
    "embedding_model = SentenceTransformer(embedding_model_path, trust_remote_code=True, device='cuda')\n",
    "compressor = PromptCompressor(model_name='microsoft/llmlingua-2-xlm-roberta-large-meetingbank', use_llmlingua2=True)\n",
    "wv_client = weaviate.connect_to_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b014f4-5de0-42ce-a96d-4ab8c06ea5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BooksProcessor:\n",
    "    def __init__(self, wv_client, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.wv_client = wv_client\n",
    "\n",
    "    def create_collection_if_not_exists(self, collection_name):\n",
    "        if not self.wv_client.collections.exists(collection_name):\n",
    "            self.wv_client.collections.create(\n",
    "                name=collection_name,\n",
    "                properties=[\n",
    "                    Property(name=\"chunk\", data_type=DataType.TEXT),\n",
    "                    Property(name=\"book_name\", data_type=DataType.TEXT),\n",
    "                    Property(name=\"chunk_num\", data_type=DataType.INT)\n",
    "                ],\n",
    "                #vectorizer_config=wvc.config.Configure.Vectorizer.none()\n",
    "                #vectorizer_config=[\n",
    "                    #Configure.NamedVectors.text2vec_ollama(\n",
    "                    #    name=\"book_vectorizer\",\n",
    "                    #    source_properties=[\"book_chunks\"],\n",
    "                    #    api_endpoint=\"http://ollama:11434\",\n",
    "                    #    model=self.embedding_model_name,\n",
    "                    #    vector_index_config=Configure.VectorIndex.hnsw(\n",
    "                    #        distance_metric=VectorDistances.COSINE\n",
    "                    #    )\n",
    "                    #)\n",
    "                #]\n",
    "            )\n",
    "        return self.wv_client.collections.get(collection_name)\n",
    "\n",
    "    def split_book(self, book_text, chunk_size, chunk_overlap):\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        return [i.page_content for i in splitter.create_documents([book_text])]\n",
    "\n",
    "    def process_book(self, book_name, book_txt):\n",
    "        if self.wv_client.collections.exists(book_name + '_medium_chunks'):\n",
    "            print(\"Book already exists\")\n",
    "            return\n",
    "        chunk_configs = [\n",
    "        #    ('_big_chunks', 3000, 1000),\n",
    "            ('_medium_chunks', 1000, 100),\n",
    "        #    ('_small_chunks', 400, 50)\n",
    "        ]\n",
    "        \n",
    "        for suffix, chunk_size, overlap in chunk_configs:\n",
    "            collection = self.create_collection_if_not_exists(book_name + suffix)\n",
    "            chunks = self.split_book(book_txt, chunk_size, overlap)\n",
    "            embeddings = self.embedding_model.encode(['search_document: ' + i for i in chunks], batch_size=15).tolist()\n",
    "            question_objs = []\n",
    "\n",
    "            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "                question_objs.append(wvc.data.DataObject(\n",
    "                    properties= {\n",
    "                        \"chunk\": chunk,\n",
    "                        \"book_name\": book_name,\n",
    "                        \"chunk_num\": i\n",
    "                    },\n",
    "                    vector=embedding\n",
    "                ))\n",
    "            collection.data.insert_many(question_objs)\n",
    "\n",
    "    def delete_book(self, book_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Delete all collections associated with a book.\n",
    "        \"\"\"\n",
    "        for suffix in ['_big_chunks', '_medium_chunks', '_small_chunks']:\n",
    "            collection_name = book_name + suffix\n",
    "            if self.wv_client.collections.exists(collection_name):\n",
    "                try:\n",
    "                    self.wv_client.collections.delete(collection_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting collection {collection_name}: {e}\")\n",
    "        print(f\"Successfully deleted collections for {book_name}\")\n",
    "\n",
    "class Search:\n",
    "    def __init__(self, wv_client, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.wv_client = wv_client\n",
    "        self.multiplier_mapping = {'_big_chunks': 0.7, '_medium_chunks': 1, '_small_chunks': 1.9}\n",
    "        #self._load_prompt_template()\n",
    "\n",
    "    def process_chunks(self, relevant_chunks):\n",
    "        relevant_text = '\\n'.join([f'\\nCHUNK {i.properties['chunk_num']}\\n' + i.properties['chunk'].strip() for i in relevant_chunks])\n",
    "        print(f'Len of relevant text: {len(relevant_text)}')\n",
    "\n",
    "    def search(self, query, book_name):\n",
    "        collection_type = '_medium_chunks'\n",
    "        print(f'Collection type: {collection_type}')\n",
    "        book = self.wv_client.collections.get(book_name + collection_type)\n",
    "        \n",
    "        total_count = book.aggregate.over_all(total_count=True).total_count\n",
    "        chunks_to_retrieve = floor(np.maximum(self.multiplier_mapping[collection_type] * np.log(total_count), 1))\n",
    "        print(f\"Retrieving {chunks_to_retrieve} chunks from book {book_name}\")\n",
    "        \n",
    "        embedding = self.embedding_model.encode('search_query: ' + query, batch_size=1)\n",
    "        response = book.query.near_vector(near_vector=list(embedding), limit=chunks_to_retrieve, return_metadata=wvc.query.MetadataQuery(certainty=True))\n",
    "        relevant_chunks = response.objects#sorted(response.objects, key=lambda x: x.properties['chunk_num'])\n",
    "        return relevant_chunks\n",
    "\n",
    "    def search_multiple_books(self, query, book_names):\n",
    "        result = []\n",
    "        for book_name in book_names:\n",
    "            chunks = self.search(query, book_name)\n",
    "            result.extend([{'chunk': i.properties['chunk'].strip(),\n",
    "                            'chunk_num': i.properties['chunk_num'],\n",
    "                            'book_name': book_name} for i in chunks])\n",
    "        return result\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, wv_client, embedding_model, compressor, llm_name, prompts_folder, compression_rate=0.75):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.searcher = Search(wv_client, self.embedding_model)\n",
    "        self.compression_rate = compression_rate\n",
    "        self.compressor = compressor\n",
    "        self.llm = OllamaLLM(\n",
    "            model=llm_name,\n",
    "            temperature=0,\n",
    "            base_url=f\"http://localhost:11434\"\n",
    "        )\n",
    "        with open(os.path.join(prompts_folder, 'final_prompt.j2')) as f:\n",
    "            self._template = f.read()\n",
    "\n",
    "    def query(self, query: str, book_names: List[str], \n",
    "             dialogue_history: Optional[List[Dict[str, str]]] = None) -> str:\n",
    "        dialogue_history = dialogue_history or []\n",
    "        compressed_contexts = []\n",
    "        \n",
    "        for book_name in book_names:\n",
    "            context = self.searcher.search(query, book_name)\n",
    "            if context:\n",
    "                compressed = self.compressor.compress_prompt(\n",
    "                    context,\n",
    "                    rate=self.compression_rate,\n",
    "                    force_tokens=['\\n', '?', '.', '!', 'CHUNK']\n",
    "                )['compressed_prompt']\n",
    "                compressed_contexts.append(f\"From {book_name}:\\n{compressed}\")\n",
    "        \n",
    "        if not compressed_contexts:\n",
    "            return \"No relevant information found.\"\n",
    "\n",
    "        print(f'Len of compressed context: {sum([len(i) for i in compressed_contexts])}')\n",
    "        final_prompt = Template(self._template).render(\n",
    "            contexts=compressed_contexts,\n",
    "            dialogue_history=dialogue_history,\n",
    "            query=query\n",
    "        )\n",
    "        \n",
    "        return self.llm.invoke(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321160a-077e-4472-b7f8-587cc225aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BooksProcessor(wv_client, embedding_model)\n",
    "with open('Sherlock Study in Scarlet.txt', 'r', encoding='utf8') as file:\n",
    "    text = file.read()\n",
    "processor.process_book('Sherlock_Study_in_Scarlet', text)\n",
    "#processor.delete_book('Sherlock_Study_in_Scarlet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1842c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection type: _medium_chunks\n",
      "Retrieving 5 chunks\n",
      "Len of relevant text: 4644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCHUNK 114\\nThe _Standard_ commented upon the fact that lawless outrages of the\\nsort usually occurred under a Liberal Administration. They arose from\\nthe unsettling of the minds of the masses, and the consequent weakening\\nof all authority. The deceased was an American gentleman who had been\\nresiding for some weeks in the Metropolis. He had stayed at the\\nboarding-house of Madame Charpentier, in Torquay Terrace, Camberwell.\\nHe was accompanied in his travels by his private secretary, Mr. Joseph\\nStangerson. The two bade adieu to their landlady upon Tuesday, the 4th\\ninst., and departed to Euston Station with the avowed intention of\\ncatching the Liverpool express. They were afterwards seen together upon\\nthe platform. Nothing more is known of them until Mr. Drebber’s body\\nwas, as recorded, discovered in an empty house in the Brixton Road,\\nmany miles from Euston. How he came there, or how he met his fate, are\\nquestions which are still involved in mystery. Nothing is known of the\\n\\nCHUNK 52\\n“There has been a bad business during the night at 3, Lauriston\\nGardens, off the Brixton Road. Our man on the beat saw a light there\\nabout two in the morning, and as the house was an empty one, suspected\\nthat something was amiss. He found the door open, and in the front\\nroom, which is bare of furniture, discovered the body of a gentleman,\\nwell dressed, and having cards in his pocket bearing the name of ‘Enoch\\nJ. Drebber, Cleveland, Ohio, U.S.A.’ There had been no robbery, nor is\\nthere any evidence as to how the man met his death. There are marks of\\nblood in the room, but there is no wound upon his person. We are at a\\nloss as to how he came into the empty house; indeed, the whole affair\\nis a puzzler. If you can come round to the house any time before\\ntwelve, you will find me there. I have left everything _in statu quo_\\nuntil I hear from you. If you are unable to come I shall give you\\nfuller details, and would esteem it a great kindness if you would\\nfavour me with your opinion.\\n\\nCHUNK 305\\n“The blood had been streaming from my nose, but I had taken no notice\\nof it. I don’t know what it was that put it into my head to write upon\\nthe wall with it. Perhaps it was some mischievous idea of setting the\\npolice upon a wrong track, for I felt light-hearted and cheerful. I\\nremembered a German being found in New York with RACHE written up above\\nhim, and it was argued at the time in the newspapers that the secret\\nsocieties must have done it. I guessed that what puzzled the New\\nYorkers would puzzle the Londoners, so I dipped my finger in my own\\nblood and printed it on a convenient place on the wall. Then I walked\\ndown to my cab and found that there was nobody about, and that the\\nnight was still very wild. I had driven some distance when I put my\\nhand into the pocket in which I usually kept Lucy’s ring, and found\\nthat it was not there. I was thunderstruck at this, for it was the only\\nmemento that I had of her. Thinking that I might have dropped it when I\\n\\nCHUNK 137\\n“I have no objection,” Lestrade answered, seating himself. “I freely\\nconfess that I was of the opinion that Stangerson was concerned in the\\ndeath of Drebber. This fresh development has shown me that I was\\ncompletely mistaken. Full of the one idea, I set myself to find out\\nwhat had become of the Secretary. They had been seen together at Euston\\nStation about half-past eight on the evening of the third. At two in\\nthe morning Drebber had been found in the Brixton Road. The question\\nwhich confronted me was to find out how Stangerson had been employed\\nbetween 8.30 and the time of the crime, and what had become of him\\nafterwards. I telegraphed to Liverpool, giving a description of the\\nman, and warning them to keep a watch upon the American boats. I then\\nset to work calling upon all the hotels and lodging-houses in the\\nvicinity of Euston. You see, I argued that if Drebber and his companion\\nhad become separated, the natural course for the latter would be to put\\n\\nCHUNK 133\\n“What is your theory, then?”\\n\\n“Well, my theory is that he followed Drebber as far as the Brixton\\nRoad. When there, a fresh altercation arose between them, in the course\\nof which Drebber received a blow from the stick, in the pit of the\\nstomach, perhaps, which killed him without leaving any mark. The night\\nwas so wet that no one was about, so Charpentier dragged the body of\\nhis victim into the empty house. As to the candle, and the blood, and\\nthe writing on the wall, and the ring, they may all be so many tricks\\nto throw the police on to the wrong scent.”\\n\\n“Well done!” said Holmes in an encouraging voice. “Really, Gregson, you\\nare getting along. We shall make something of you yet.”'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = Search(wv_client, embedding_model)\n",
    "#rag_context = search.search(query='search_query: ' + 'What happened in London?', book_name='Sherlock_Study_in_Scarlet')\n",
    "#rag_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c86d91a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ustin\\AppData\\Local\\Temp\\ipykernel_14788\\1451993357.py:1: ResourceWarning: unclosed <socket.socket fd=5060, family=23, type=1, proto=0, laddr=('::1', 58202, 0, 0), raddr=('::1', 11434, 0, 0)>\n",
      "  rag = RAGSystem(wv_client, embedding_model, compressor, llm_name=llm_name, prompts_folder=prompts_folder)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection type: _medium_chunks\n",
      "Retrieving 5 chunks\n",
      "Len of relevant text: 4645\n",
      "Len of compressed context: 3696\n",
      "According to CHUNK 70, the single word \"RACHE\" was written in blood-red letters on the wall near Enoch Drebber's body.\n",
      "Collection type: _medium_chunks\n",
      "Retrieving 5 chunks\n",
      "Len of relevant text: 4450\n",
      "Len of compressed context: 3551\n",
      "According to CHUNK 64, Sherlock Holmes found numerous gouts and splashes of blood around the body, indicating that there was a second individual involved, presumably the murderer.\n",
      "Collection type: _medium_chunks\n",
      "Retrieving 5 chunks\n",
      "Len of relevant text: 4553\n",
      "Len of compressed context: 3645\n",
      "According to CHUNK 12, when introducing himself, Dr. Watson asked how Sherlock Holmes knew that he had been in Afghanistan. Sherlock Holmes replied that he had noticed a scar on Dr. Watson's hand, which suggested that he had been in Afghanistan (CHUNK 2 mentions that Dr. Watson completed his studies attached to the Fifth Northumberland Fusiliers as Assistant Surgeon before joining the second Afghan war).\n",
      "Collection type: _medium_chunks\n",
      "Retrieving 5 chunks\n",
      "Len of relevant text: 4151\n",
      "Len of compressed context: 3302\n",
      "Based on the context provided, it is revealed that the actual murderer of Enoch Drebber was John Ferrier, a former rival in love. The motive for the murder appears to be related to an old romantic feud between Ferrier and Drebber, as well as their involvement with the Latter Day Saints (Mormonism).\n",
      "Collection type: _medium_chunks\n",
      "Retrieving 5 chunks\n",
      "Len of relevant text: 3566\n",
      "Len of compressed context: 2832\n",
      "According to the context:\n",
      "\n",
      "* Inspectors Gregson and Lestrade were involved in the investigation, but their approach was described as \"conventional\" and \"shockingly so\", implying they might not be using unconventional methods like Sherlock Holmes.\n",
      "* They were initially skeptical of Holmes' abilities, with Inspector Lestrade thinking he had \"gone off on a wrong track\".\n",
      "* However, after Holmes shared his clue, Gregson laughed and seemed impressed, while Lestrade was crest-fallen when he realized he didn't know where the American's antecedents were.\n",
      "* It is mentioned that they will be presented with a testimonial by Sherlock Holmes as recognition of their services.\n",
      "\n",
      "No contradictions are found between different sources.\n"
     ]
    }
   ],
   "source": [
    "rag = RAGSystem(wv_client, embedding_model, compressor, llm_name=llm_name, prompts_folder=prompts_folder)\n",
    "\n",
    "queries = [\n",
    "    \"What word was written in blood on the wall near Enoch Drebber's body?\",\n",
    "    \"What clue did Sherlock Holmes find near the body that aided in the investigation?\",\n",
    "    \"What method did Sherlock Holmes use to deduce Dr. Watson's profession and background upon their first meeting?\",\n",
    "    \"Who was the actual murderer of Enoch Drebber, and what was the motive?\",\n",
    "    \"What roles did Inspectors Gregson and Lestrade from Scotland Yard play in the investigation?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    response = rag.query(\n",
    "        query=query,\n",
    "        book_names=['Sherlock_Study_in_Scarlet'],\n",
    "        dialogue_history=[]\n",
    "    )\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28194178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ustin\\AppData\\Local\\Temp\\ipykernel_14788\\3272381286.py:20: ResourceWarning: unclosed <socket.socket fd=5944, family=23, type=1, proto=0, laddr=('::1', 53385, 0, 0), raddr=('::1', 11434, 0, 0)>\n",
      "  llm = OllamaLLM(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    user_question: str\n",
    "    current_question: str\n",
    "    all_questions: str\n",
    "    facts: List[str]\n",
    "    watched_documents: List[int]\n",
    "\n",
    "retriever = Search(wv_client, embedding_model)\n",
    "llm = OllamaLLM(\n",
    "            model=llm_name,\n",
    "            temperature=0,\n",
    "            base_url=f\"http://localhost:11434\")\n",
    "book_names = ['Sherlock_Study_in_Scarlet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5baa6011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: {\"binary_score\": \"yes\"}\n",
      "Parsed successfully: {'binary_score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize the model\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# Input data\n",
    "question = \"agent memory\"\n",
    "doc_txt = \"This document discusses agent memory in multi-agent systems.\"\n",
    "\n",
    "# Define the prompt\n",
    "prompt = (\n",
    "    \"Evaluate the relevance of the document to the user's question. Respond strictly in JSON format:\\n\"\n",
    "    '{\"binary_score\": \"yes\"} or {\"binary_score\": \"no\"}.\\n\\n'\n",
    "    \"Example:\\n\"\n",
    "    'Question: What animals live in the Arctic?\\n'\n",
    "    'Document: The Arctic is home to polar bears and walruses.\\n'\n",
    "    'Answer: {\"binary_score\": \"yes\"}\\n\\n'\n",
    "    f\"Question: {question}\\nDocument: {doc_txt}\\n\"\n",
    ")\n",
    "\n",
    "# Get the model's response\n",
    "response = llm.invoke(prompt)\n",
    "print(\"Model response:\", response)\n",
    "\n",
    "# Validate and parse the output\n",
    "from json import loads\n",
    "\n",
    "try:\n",
    "    parsed_response = loads(response)\n",
    "    if parsed_response.get(\"binary_score\") in [\"yes\", \"no\"]:\n",
    "        print(\"Parsed successfully:\", parsed_response)\n",
    "    else:\n",
    "        print(\"Error: Invalid 'binary_score' value.\")\n",
    "except Exception as e:\n",
    "    print(f\"Parsing error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae303472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = 'search_query: ' + state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.search_multiple_books(question, book_names=book_names)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def collect_info(state):\n",
    "    print(\"---COLLECTING INFO---\")\n",
    "    question = 'search_query: ' + state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.search_multiple_books(question, book_names=book_names)\n",
    "    return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
