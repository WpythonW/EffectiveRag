{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "ollama_url = 'localhost'\n",
    "model = 'llama3.2'\n",
    "llm = ChatOllama(model=model, base_url=f\"{ollama_url}:11434\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alice_in_wonderland.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=700,  # Максимальный размер чанка\n",
    "    chunk_overlap=100  # Перекрытие между чанками\n",
    ")\n",
    "\n",
    "documents = splitter.create_documents([text], metadatas=[{'book': 'alice_in_wonderland'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\",  base_url=f\"{ollama_url}:11434\"),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'book': 'alice_in_wonderland'}, page_content=\"`Soo--oop of the e--e--evening,\\n        Beautiful, beautiful Soup!'\\n\\n\\n\\n= CHAPTER XI =\\n=( Who Stole the Tarts? )=\\n\\n\\n  The King and Queen of Hearts were seated on their throne when\\nthey arrived, with a great crowd assembled about them--all sorts\\nof little birds and beasts, as well as the whole pack of cards:\\nthe Knave was standing before them, in chains, with a soldier on\\neach side to guard him; and near the King was the White Rabbit,\\nwith a trumpet in one hand, and a scroll of parchment in the\\nother.  In the very middle of the court was a table, with a large\\ndish of tarts upon it:  they looked so good, that it made Alice\\nquite hungry to look at them--`I wish they'd get the trial done,'\\nshe thought, `and hand round the refreshments!'  But there seemed\\nto be no chance of this, so she began looking at everything about\\nher, to pass away the time.\\n\\n  Alice had never been in a court of justice before, but she had\\nread about them in books, and she was quite pleased to find that\\nshe knew the name of nearly everything there.  `That's the\\njudge,' she said to herself, `because of his great wig.'\\n\\n  The judge, by the way, was the King; and as he wore his crown\\nover the wig, (look at the frontispiece if you want to see how he\\ndid it,) he did not look at all comfortable, and it was certainly\\nnot becoming.\\n\\n  `And that's the jury-box,' thought Alice, `and those twelve\\ncreatures,' (she was obliged to say `creatures,' you see, because\\nsome of them were animals, and some were birds,) `I suppose they\\nare the jurors.'  She said this last word two or three times over\\nto herself, being rather proud of it:  for she thought, and\\nrightly too, that very few little girls of her age knew the\\nmeaning of it at all.  However, `jury-men' would have done just\\nas well.\\n\\n  The twelve jurors were all writing very busily on slates.\\n`What are they doing?'  Alice whispered to the Gryphon.  `They\\ncan't have anything to put down yet, before the trial's begun.'\\n\\n  `They're putting down their names,' the Gryphon whispered in\\nreply, `for fear they should forget them before the end of the\\ntrial.'\\n\\n  `Stupid things!' Alice began in a loud, indignant voice, but\\nshe stopped hastily, for the White Rabbit cried out, `Silence in\\nthe court!' and the King put on his spectacles and looked\\nanxiously round, to make out who was talking.\"),\n",
       " Document(metadata={'book': 'alice_in_wonderland'}, page_content=\"Luckily for Alice, the little magic bottle had now had its full\\neffect, and she grew no larger:  still it was very uncomfortable,\\nand, as there seemed to be no sort of chance of her ever getting\\nout of the room again, no wonder she felt unhappy.\\n\\n  `It was much pleasanter at home,' thought poor Alice, `when one\\nwasn't always growing larger and smaller, and being ordered about\\nby mice and rabbits.  I almost wish I hadn't gone down that\\nrabbit-hole--and yet--and yet--it's rather curious, you know,\\nthis sort of life!  I do wonder what CAN have happened to me!\\nWhen I used to read fairy-tales, I fancied that kind of thing\\nnever happened, and now here I am in the middle of one!  There\\nought to be a book written about me, that there ought!  And when\\nI grow up, I'll write one--but I'm grown up now,' she added in a\\nsorrowful tone; `at least there's no room to grow up any more\\nHERE.'\\n\\n  `But then,' thought Alice, `shall I NEVER get any older than I\\nam now?  That'll be a comfort, one way--never to be an old woman--\\nbut then--always to have lessons to learn!  Oh, I shouldn't like THAT!'\\n\\n  `Oh, you foolish Alice!' she answered herself.  `How can you\\nlearn lessons in here?  Why, there's hardly room for YOU, and no\\nroom at all for any lesson-books!'\\n\\n  And so she went on, taking first one side and then the other,\\nand making quite a conversation of it altogether; but after a few\\nminutes she heard a voice outside, and stopped to listen.\\n\\n  `Mary Ann!  Mary Ann!' said the voice.  `Fetch me my gloves\\nthis moment!'  Then came a little pattering of feet on the\\nstairs.  Alice knew it was the Rabbit coming to look for her, and\\nshe trembled till she shook the house, quite forgetting that she\\nwas now about a thousand times as large as the Rabbit, and had no\\nreason to be afraid of it.\\n\\n  Presently the Rabbit came up to the door, and tried to open it;\\nbut, as the door opened inwards, and Alice's elbow was pressed\\nhard against it, that attempt proved a failure.  Alice heard it\\nsay to itself `Then I'll go round and get in at the window.'\\n\\n  `THAT you won't' thought Alice, and, after waiting till she\\nfancied she heard the Rabbit just under the window, she suddenly\\nspread out her hand, and made a snatch in the air.  She did not\\nget hold of anything, but she heard a little shriek and a fall,\\nand a crash of broken glass, from which she concluded that it was\\njust possible it had fallen into a cucumber-frame, or something\\nof the sort.\"),\n",
       " Document(metadata={'book': 'alice_in_wonderland'}, page_content=\"`I can't go no lower,' said the Hatter:  `I'm on the floor, as\\nit is.'\\n\\n  `Then you may SIT down,' the King replied.\\n\\n  Here the other guinea-pig cheered, and was suppressed.\\n\\n  `Come, that finished the guinea-pigs!' thought Alice.  `Now we\\nshall get on better.'\\n\\n  `I'd rather finish my tea,' said the Hatter, with an anxious\\nlook at the Queen, who was reading the list of singers.\\n\\n  `You may go,' said the King, and the Hatter hurriedly left the\\ncourt, without even waiting to put his shoes on.\\n\\n  `--and just take his head off outside,' the Queen added to one\\nof the officers:  but the Hatter was out of sight before the\\nofficer could get to the door.\\n\\n  `Call the next witness!' said the King.\\n\\n  The next witness was the Duchess's cook.  She carried the\\npepper-box in her hand, and Alice guessed who it was, even before\\nshe got into the court, by the way the people near the door began\\nsneezing all at once.\\n\\n  `Give your evidence,' said the King.\\n\\n  `Shan't,' said the cook.\\n\\n  The King looked anxiously at the White Rabbit, who said in a\\nlow voice, `Your Majesty must cross-examine THIS witness.'\\n\\n  `Well, if I must, I must,' the King said, with a melancholy\\nair, and, after folding his arms and frowning at the cook till\\nhis eyes were nearly out of sight, he said in a deep voice, `What\\nare tarts made of?'\\n\\n  `Pepper, mostly,' said the cook.\\n\\n  `Treacle,' said a sleepy voice behind her.\\n\\n  `Collar that Dormouse,' the Queen shrieked out.  `Behead that\\nDormouse!  Turn that Dormouse out of court!  Suppress him!  Pinch\\nhim!  Off with his whiskers!'\\n\\n  For some minutes the whole court was in confusion, getting the\\nDormouse turned out, and, by the time they had settled down\\nagain, the cook had disappeared.\\n\\n  `Never mind!' said the King, with an air of great relief.\\n`Call the next witness.'  And he added in an undertone to the\\nQueen, `Really, my dear, YOU must cross-examine the next witness.\\nIt quite makes my forehead ache!'\\n\\n  Alice watched the White Rabbit as he fumbled over the list,\\nfeeling very curious to see what the next witness would be like,\\n`--for they haven't got much evidence YET,' she said to herself.\\nImagine her surprise, when the White Rabbit read out, at the top\\nof his shrill little voice, the name `Alice!'\\n\\n\\n\\n= CHAPTER XII =\\n=( Alice's Evidence )=\"),\n",
       " Document(metadata={'book': 'alice_in_wonderland'}, page_content=\"Oh dear, what nonsense I'm talking!'\\n\\n  Just then her head struck against the roof of the hall:  in\\nfact she was now more than nine feet high, and she at once took\\nup the little golden key and hurried off to the garden door.\\n\\n  Poor Alice!  It was as much as she could do, lying down on one\\nside, to look through into the garden with one eye; but to get\\nthrough was more hopeless than ever:  she sat down and began to\\ncry again.\\n\\n  `You ought to be ashamed of yourself,' said Alice, `a great\\ngirl like you,' (she might well say this), `to go on crying in\\nthis way!  Stop this moment, I tell you!'  But she went on all\\nthe same, shedding gallons of tears, until there was a large pool\\nall round her, about four inches deep and reaching half down the\\nhall.\\n\\n  After a time she heard a little pattering of feet in the\\ndistance, and she hastily dried her eyes to see what was coming.\\nIt was the White Rabbit returning, splendidly dressed, with a\\npair of white kid gloves in one hand and a large fan in the\\nother:  he came trotting along in a great hurry, muttering to\\nhimself as he came, `Oh! the Duchess, the Duchess! Oh! won't she\\nbe savage if I've kept her waiting!'  Alice felt so desperate\\nthat she was ready to ask help of any one; so, when the Rabbit\\ncame near her, she began, in a low, timid voice, `If you please,\\nsir--'  The Rabbit started violently, dropped the white kid\\ngloves and the fan, and skurried away into the darkness as hard\\nas he could go.\\n\\n  Alice took up the fan and gloves, and, as the hall was very\\nhot, she kept fanning herself all the time she went on talking:\\n`Dear, dear!  How queer everything is to-day!  And yesterday\\nthings went on just as usual.  I wonder if I've been changed in\\nthe night?  Let me think:  was I the same when I got up this\\nmorning?  I almost think I can remember feeling a little\\ndifferent.  But if I'm not the same, the next question is, Who in\\nthe world am I?  Ah, THAT'S the great puzzle!'  And she began\\nthinking over all the children she knew that were of the same age\\nas herself, to see if she could have been changed for any of\\nthem.\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(query='rabbit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\RAG project\\venv\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text appears to be a passage from the book \"Alice's Adventures in Wonderland\" by Lewis Carroll. The passage describes Alice's adventures with the Gryphon and the Mock Turtle, including their conversation about the Mock Turtle's history.\n",
      "\n",
      "Specifically, the passage begins with the Gryphon and Alice approaching the Mock Turtle, who is sitting on a rock and sobbing heavily. The Gryphon introduces Alice to the Mock Turtle, and they all sit down together. The Mock Turtle then begins to tell his story, explaining that he was once a real turtle but has since become a sea creature.\n",
      "\n",
      "The passage also includes some of the distinctive language and humor characteristic of Carroll's writing style, such as the use of nonsensical phrases (\"Hjckrrh!\") and absurd situations (e.g. the idea of going to school in the sea).\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='no')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat information does the book contain regarding the concept of agent memory?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "# LLM\n",
    "\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an the book content question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning. Write only new question without any additional text.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            #print(d)\n",
    "            #input()\n",
    "            continue\n",
    "    print(len(filtered_docs))\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m workflow \u001b[38;5;241m=\u001b[39m StateGraph(GraphState)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define the nodes\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieve\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mretrieve\u001b[49m)  \u001b[38;5;66;03m# retrieve\u001b[39;00m\n\u001b[0;32m      7\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrade_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m, grade_documents)  \u001b[38;5;66;03m# grade documents\u001b[39;00m\n\u001b[0;32m      8\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate\u001b[39m\u001b[38;5;124m\"\u001b[39m, generate)  \u001b[38;5;66;03m# generatae\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retrieve' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"who is bill\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
